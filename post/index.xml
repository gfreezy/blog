<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on allsunday</title><link>http://blog.allsunday.io/post/</link><description>Recent content in Posts on allsunday</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 09 Nov 2014 00:00:00 +0000</lastBuildDate><atom:link href="http://blog.allsunday.io/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Python 多继承下 metaclass 优先级</title><link>http://blog.allsunday.io/post/2014-11-09-python-mutiple-metaclass/</link><pubDate>Sun, 09 Nov 2014 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2014-11-09-python-mutiple-metaclass/</guid><description>在 Python 里面，class 也是一个对象，它是 type 实例化后生成的对象。 我们一般用如下的代码来定义一个类。
class ClassA(object): def method1(self): pass @classmethod def method2(cls): pass 实际上 class 这个关键字是类似语法糖一样的东西。 上面的 class 定义等价为
def method1(self): pass @classmethod def method2(cls): pass attrs = { 'method1': method1, 'method2': method2, } ClassA = type('ClassA', (object,), attrs) type 的三个参数分别为：
类的名字（ ClassA.__name__ 中存储的名字),
继承的父类(可以是多个父类),
类的所有属性（类的各种方法和属性）。
以上是一个未指定 __metaclass__ 的 class 的生成过程。 但是在 ORM 的代码中，经常可以看到类似这样的写法</description></item><item><title>IPTABLES INPUT 和 PREROUTING 的区别</title><link>http://blog.allsunday.io/post/2014-05-27-iptables-input%E5%92%8Cprerouting%E7%9A%84%E5%8C%BA%E5%88%AB/</link><pubDate>Tue, 27 May 2014 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2014-05-27-iptables-input%E5%92%8Cprerouting%E7%9A%84%E5%8C%BA%E5%88%AB/</guid><description>之前一直以为 iptables 的 input 链是用来判断是否允许 ip 包进入（不管这个 ip 包的目标地址是不是本机）， prerouting 链是用来做 ip 包转发的。
我们之前的架构是网管机器 maize 通过 iptables 把 80 端口的 ip 包全部转发到内网 carrot[1] 机器的 nginx 上面。有一阵爬虫特别多， 我们就把他们在 nginx 里面直接对这些 爬虫的 ip deny了。但是经过 deny 了，这些爬虫的还是会进 nginx 的日志，影响到正常的监控。 于是我们就想干脆直接在网关的 iptables 那边 drop 掉，这样 nginx 日志也能干净不少。说干就干
iptables -A INPUT -s xx.xx.xx.xx/24 -j DROP 并把 nginx 里面的 deny 配置给删掉了，我们本以为这样整个世界就清净了。但是流量监控显示，爬虫的流量不但没有下降，反而反弹到之前的量了。 直接查看 nginx 的日志，发现这些本应该被屏蔽的 ip ，竟然都在正常爬取内容。小伙伴们都惊呆了。
为什么在 input 链里面 drop 了，还是没有生效。整个世界都不好了。用 iptables + input + drop + not work 等类似的关键字 Google 了好半天也没找出啥有用的信息，只是觉的这个应该跟 iptables 的转发应该有关系。抱着试一试的想法，在 nginx 所在的机器上面运行了这句 drop 的命令，结果生效了，爬虫被成功的屏蔽了。</description></item><item><title>Linux 磁盘性能测试工具</title><link>http://blog.allsunday.io/post/2014-05-27-linux%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/</link><pubDate>Tue, 27 May 2014 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2014-05-27-linux%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/</guid><description>公司的 MySQL io性能遇到了瓶颈， MySQL 层面的优化基本已经都做完了， 而应用层面的优化又会过于繁琐与复杂。 于是我们最初的结论是最简单的方法就是上 SSD。后来询问了豆瓣flex关于SSD的经验， 发现SSD还不是那么成熟，出现问题的情况还是比较多，而我们又没有这方面的经验， 所以就把SSD这个方案给搁置了。
豆瓣flex给出豆瓣这方面的经验是：用15000转的硬盘做RAID 10。使用RAID卡的 缓存以后，性能相对于直接使用硬盘有极大的提升。而且RAID的技术已经非常的成熟，这个方案看起来是比较靠谱的。
我们的机器是 R710，默认自带的RAID卡性能非常弱，只能做 RAID 0或 RAID 1。flex推荐的是 H700，因为 R710 的背板线与 H700不兼容，所以连线也要一起换掉。
做完RAID以后，第一件事情就是要测试下磁盘的性能。常用的工具有 hdparm 和 dd，但是这两个测试的都是顺序读写，而实际的使用情景很大一部分都是随机读写。flex推荐了一个工具 fio，一个专门用来测试硬盘io的工具。
有一篇文章很好的介绍了它的使用方法： http://www.linux.com/learn/tutorials/442451-inspecting-disk-io-performance-with-fio/
我只用了它最基本的测试随机读的功能。在RAID1 10上测试的结果是平均读取的速度到了 80M/s。而我在现有的服务器上面（未做RAID，7500转硬盘），这个数值只有 1M/s 左右。没做15000转硬盘的性能测试，所以不是很清楚 RAID 10 得到的性能提升有多少。</description></item><item><title>moosefs 新部署 chunkserver</title><link>http://blog.allsunday.io/post/2014-05-12-moosefs%E6%96%B0%E9%83%A8%E7%BD%B2chunkserver/</link><pubDate>Mon, 12 May 2014 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2014-05-12-moosefs%E6%96%B0%E9%83%A8%E7%BD%B2chunkserver/</guid><description>在 /etc/apt/source.lst 里面加入
deb http://apt-repo:3143/ / 运行一下命令来安装 moosefs 相关的包
apt-get update apt-get install moosefs-chunkserver moosefs-client moosefs-master 修改配置文件
cd /etc/mfs cp mfschunkserver.cfg.dist mfschunkserver.cfg 因为我在 DNS 里面配置了 mfsmaster 指向 master 的 ip ，所以我们 不需要对 mfschunkserver 的配置进行修改。
cd /etc/mfs cp mfshdd.cfg.dist mfshdd.cfg vim mfshdd.cfg 在 mfshdd.cfg 里面加入存放数据的目录。
增加启动脚本
wget https://gist.githubusercontent.com/gfreezy/f1dc2ee1620b2cbe970f/raw/65dce19ebd768571ce0f258916743ca4999e11fb/mfschunkserver -O /etc/init.d/mfschunkserver chmod +x /etc/init.d/mfschunkserver update-rc.d mfschunkserver defaults 启动 mfschunkserver</description></item><item><title>MySQL 性能问题排查</title><link>http://blog.allsunday.io/post/2014-04-17-mysql%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</link><pubDate>Thu, 17 Apr 2014 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2014-04-17-mysql%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</guid><description>一月份上线了一个新的APP版本后导致MySQL的性能急剧下降。机器的io近几天基本一直都是 100% 状态，于是开始调MySQL的性能。
慢日志 首先查看的时慢日志。因为近几天io都是100%，每秒的慢日志已经到了几十的量级，由于io的下降，导致很多原本不应该慢的查询也都在慢查询日志里面。肉眼已经很难看了。 这时，就要使用percona的工具，pt-query-digest了。
pt-query-digest /var/log/mysql/mysql-slow.log | vim - 分析结果会在vim里面打开。pt-query-digest会根据查询花费的时间排序，按照顺序往下看，一个一个的看，是不是该建的索引没建，或者建的不合适，导致 查询没用上索引。
show processlist 慢日志一般能够解决很大的一部分问题，但是有些查询很拖累性能，但是他单条查询的时间并不长，不会出现在慢日志中。
这时候，一般可以用 pt-query-digest 的 --processlist 参数，对 MySQL 以一定的时间间隔执行 show processlist。
pt-query-digest --user user_name --password pass_word --processlist localhost --interval 0.01 --run-time 10m | vim - 抽样分析的结果同样会在 vim 中显示。 依次分析每条语句。
tcpdump 如果 show processlist 还是没办法找到性能瓶颈，那就只好使用 tcpdump ，抓取 MySQL 的所有查询，放到 pt-query-digest 里面分析了。</description></item><item><title>mako根据条件判断是否使用页面缓存</title><link>http://blog.allsunday.io/post/2014-02-19-cache-pages-for-anonymous-users-in-mako/</link><pubDate>Wed, 19 Feb 2014 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2014-02-19-cache-pages-for-anonymous-users-in-mako/</guid><description>最近遇到网站速度慢的情况，排查许久没查出什么原因。于是想着匿名用户的访问量占据了一半多，如果这一部分的请求全部缓存下来，那么应该能够很大程度上提升网站的响应速度。 之前已经在一些页面里面使用了 Mako 的页面缓存，具体的文档可以查看 Mako 官网
# CacheImepl的定义 class CMemcachedImpl(CacheImpl): def __init__(self, cache): from mysite.model.init_db import page_mc as mc super(CMemcachedImpl, self).__init__(cache) self.mc = mc def get_or_create(self, key, creation_function, **kw): value = self.mc.get(key) if not value: value = creation_function() timeout = kw.get('timeout', 60) try: timeout = int(timeout) except ValueError: timeout = 60 # 防止缓存雪崩，即大量待缓存在同一时刻失效 timeout = timeout + random.randint(0, timeout / 10) self.mc.set(key, value, timeout) return value def set(self, key, value, **kw): timeout = kw.</description></item><item><title>制作 Moosefs debian 安装包</title><link>http://blog.allsunday.io/post/2013-07-17-%E5%88%B6%E4%BD%9Cmoosefs-debian%E5%AE%89%E8%A3%85%E5%8C%85/</link><pubDate>Wed, 17 Jul 2013 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2013-07-17-%E5%88%B6%E4%BD%9Cmoosefs-debian%E5%AE%89%E8%A3%85%E5%8C%85/</guid><description>Moosefs一共可以分为 master, chunkserver, client 三个应用程序，需要分别制作3个debian安装包。
准备工作 确保 fuse 已经安装
安装 checkinstall
apt-get install checkinstall 从 moosefs 下载源代码 mfs-1.6.27.tar.gz。
创建用于编译的mfs帐号和用户组
groupadd mfs useradd -g mfs mfs 解压源代码，并切换目录
tar xvzf mfs-1.6.27.tar.gz cd mfs-1.6.27 制作 Master 安装包 配置
./configure --prefix=/usr --sysconfdir=/etc --localstatedir=/var/lib --with-default-user=mfs --with-default-group=mfs --disable-mfschunkserver --disable-mfsmount 编译, -j4 是为了让 make 充分利用多核CPU</description></item><item><title>MySQL replica</title><link>http://blog.allsunday.io/post/2012-10-23-mysql-replica/</link><pubDate>Fri, 30 Nov 2012 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2012-10-23-mysql-replica/</guid><description>&lt;p>昨天终于把拖了很久的MySQL replica给做了。一共有两台服务器，一台是MySQL master，hostname是fig，还有一台是即将作为slave的，hostname为pistachio。两台机器装有相同版本的MySQL。&lt;/p>
&lt;h2 id="基本步骤">基本步骤&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>开启master fig的 &lt;em>binary log&lt;/em> ，修改 &lt;em>my.cnf&lt;/em> ，增加如下的配置&lt;/p>
&lt;pre>&lt;code> [mysqld]
log-bin=mysql-bin
server-id=1
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>在master上面为replica新建一个账号&lt;/p>
&lt;pre>&lt;code> mysql&amp;gt; CREATE USER 'repl'@'%.mydomain.com' IDENTIFIED BY 'slavepass';
mysql&amp;gt; GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%.mydomain.com';
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol></description></item><item><title>apt-get 回滚 testing 到某一天的状态</title><link>http://blog.allsunday.io/post/2012-10-17-apt-gethui-gun-testingdao-mou-tian-de-zhuang-tai/</link><pubDate>Wed, 17 Oct 2012 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2012-10-17-apt-gethui-gun-testingdao-mou-tian-de-zhuang-tai/</guid><description>&lt;p>公司的服务器用的是Debian的testing版本，上面的MySQL是在3月份装的，是5.1的版本。因为最近要给MySQL加一个cluster，所以需要在新的虚拟机C上安装MySQL环境。&lt;/p>
&lt;p>服务器某台A虚拟机安装了apt-cache-ng，所有其余的机器的apt源都指向这台机器。由于我的手欠，在A上面执行了&lt;/p>
&lt;pre>&lt;code>apt-get update
&lt;/code>&lt;/pre>
&lt;p>导致testing库更新到了最新的代码。&lt;/p>
&lt;p>然后，纠结的地方到了，apt-get安装到的MySQL是5.5版本。我能做的只有两个方法：&lt;/p>
&lt;ol>
&lt;li>把生产环境中5.1的MySQL升级&lt;/li>
&lt;li>想办法安装5.1版本的MySQL&lt;/li>
&lt;/ol></description></item><item><title>nginx location and proxy_pass</title><link>http://blog.allsunday.io/post/2012-09-12-nginx-location-and-proxy-pass/</link><pubDate>Wed, 12 Sep 2012 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2012-09-12-nginx-location-and-proxy-pass/</guid><description>&lt;p>&lt;em>Nginx&lt;/em>的&lt;em>location&lt;/em>的格式为&lt;/p>
&lt;pre>&lt;code>location [=|~|~*|^~] uri { … }
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>=&lt;/code>表示完全匹配&lt;em>uri&lt;/em>&lt;/li>
&lt;li>&lt;code>^~&lt;/code>表示以&lt;em>uri&lt;/em>打头&lt;/li>
&lt;li>&lt;code>~&lt;/code>表示正则匹配&lt;em>uri&lt;/em>，且不区分大小写&lt;/li>
&lt;li>&lt;code>~*&lt;/code>表示正则匹配&lt;em>uri&lt;/em>，但是区分大小写&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Nginx&lt;/em>匹配&lt;em>location&lt;/em>时，有一定的优先级，&lt;code>=&lt;/code>优先级最高，其次&lt;code>^~&lt;/code>，最后是&lt;code>~&lt;/code>和&lt;code>~*&lt;/code>。同一类型的匹配按照出现的先后顺序匹配。&lt;/p>
&lt;p>下面是一个例子，来自&lt;a href="http://wiki.nginx.org/HttpCoreModule#location">http://wiki.nginx.org/HttpCoreModule#location&lt;/a>&lt;/p>
&lt;pre>&lt;code>location = / {
# matches the query / only.
[ configuration A ]
}
location / {
# matches any query, since all queries begin with /, but regular
# expressions and any longer conventional blocks will be
# matched first.
[ configuration B ]
}
location ^~ /images/ {
# matches any query beginning with /images/ and halts searching,
# so regular expressions will not be checked.
[ configuration C ]
}
location ~* \.(gif|jpg|jpeg)$ {
# matches any request ending in gif, jpg, or jpeg. However, all
# requests to the /images/ directory will be handled by
# Configuration C.
[ configuration D ]
}
&lt;/code>&lt;/pre></description></item><item><title>scrapy could not parse some html page correctly</title><link>http://blog.allsunday.io/post/2012-09-06-scrapy-could-not-parse-some-html-page-correctly/</link><pubDate>Thu, 06 Sep 2012 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2012-09-06-scrapy-could-not-parse-some-html-page-correctly/</guid><description>在用Scrapy爬取淘宝的搜索结果时，发现很多的页面通过XPathSelector转换后丢失了很多的数据，只留下了一小部分的页面。
起初是以为淘宝用了JS来动态加载搜索结果，用Chrome Develop Tool禁用了JS，发现还是没有丢失页面数据。也查了下，没有使用跳转。然后用scrapy shell url来测试这个URL，hxs.extract()的结果要比response.body中的内容少了一个数量级。
问题出在了lxml的处理上。淘宝的页面是使用GBK编码的，所以scrapy的encoding处理出了问题。Google之，终于在stackoverflow上面找到了一篇文章。按照它上面说的加上encoding处理后，问题解决了。
看样子以后爬国内的页面可要长点心了，一个页面编码问题卡了我一整天。
附上处理encoding的代码：
{% highlight python %} import charset
def parse(self, response):
encoding = chardet.detect(response.body)['encoding'] if encoding != 'utf-8': response.body = response.body.decode(encoding, 'replace').encode('utf-8') hxs = HtmlXPathSelector(response) data = hxs.select(&amp;quot;//div[@id='param-more']&amp;quot;).extract() #print encoding print data {% endhighlight %}</description></item><item><title>pyramid1.3使用mako模板</title><link>http://blog.allsunday.io/post/2012-03-28-pyramid1-dot-3shi-yong-makomo-ban/</link><pubDate>Wed, 28 Mar 2012 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2012-03-28-pyramid1-dot-3shi-yong-makomo-ban/</guid><description>Pyramid的pt模板文件可以使用相对路径来查找，如下
@view_config(route_name='home', renderer=templates/mytemplate.pt') 但是Pyramid的Mako模板不支持用相对路径，你必须使用绝对路径
@view_config(route_name='home', renderer='translatform:templates/mytemplate.mako') 或者，可以在 productioin.ini 文件里面加入 mako.directory=translatform:/ ，这样就可以用相对路径来定位mako模板了。</description></item><item><title>实习报告</title><link>http://blog.allsunday.io/post/2012-03-24-shi-xi-bao-gao/</link><pubDate>Sat, 24 Mar 2012 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2012-03-24-shi-xi-bao-gao/</guid><description>&lt;p>2011年5月，此时我大学生活的第三个年头已经走向结束，无忧无虑的时光已经
基本上过完。走出学校，走入社会的事实已经近在眼前了。为了能够让自己增长
社会实践的经历，减少与社会需求的脱节，脱离象牙塔，我开始考虑实习的问题。
这个时候来学校招实习生的公司并不多，我有幸参加了淘宝的电话面试，可惜没
有通过。不过也正因为这样，我才能够来到现在的公司”下厨房“。我开始接触”
下厨房“是从一个技术社区开始的，我在上面看到了实习招聘，正好我也在找实
习，所以我完全没有多想，就把简历发了过去。&lt;/p></description></item><item><title>从Emacs中直接操作Octopress</title><link>http://blog.allsunday.io/post/2012-01-12-cong-emacszhong-zhi-jie-cao-zuo-octopress/</link><pubDate>Thu, 12 Jan 2012 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2012-01-12-cong-emacszhong-zhi-jie-cao-zuo-octopress/</guid><description>前段时间换上了Octopress，也把博客迁到了Github上， 但是发文章变的好麻烦，又要rake new_post[title]，又要打开编辑器编辑， 然后还要rake generate &amp;amp;&amp;amp; rake deploy。而且我用zsh，和rvm还不兼容， 每次都要重新开个terminal，用bash来操作。
今天无意间看到了rbenv，rvm的等 价物，也可以实现ruby的版本管理。看了下，支持zsh，立马装上。切换bash的 问题解决了。
然后开始琢磨Octopress和Emacs的集成的问题。我的想法是直接从Emacs里面新 建、上传、部署，然后Google后找到了 这个。 正好昨天看了会Elisp，今天用上了。仿照着它的代码，我自己写了一个更加完整的。
代码上传到了Github: https://github.com/gfreezy/octopress-emacs</description></item><item><title>emacs支持中文输入</title><link>http://blog.allsunday.io/post/2011-12-05-emacszhi-chi-zhong-wen-shu-ru/</link><pubDate>Mon, 05 Dec 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-12-05-emacszhi-chi-zhong-wen-shu-ru/</guid><description>&lt;p>自己手动编译Emacs24，加入了gtk3的支持。因为我是&lt;code>en_US.utf8&lt;/code> 的locale，
Emacs默认执行时是不支持ibus的中文输入。必须要 &lt;code>LC_CTYPE=zh_CN.utf8 emacs&lt;/code>
这样指定locale为zh_CN才能支持中文输入。因为每次都要输入很麻烦，而且从
菜单启动的时候没办法指定。&lt;/p>
&lt;p>所以找了个最简单的方法：&lt;/p></description></item><item><title>Move wicd's icon to gnome-shell's status bar(top bar)</title><link>http://blog.allsunday.io/post/2011-12-05-move-wicds-icon-to-gnome-shells-status-bar-top-bar/</link><pubDate>Mon, 05 Dec 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-12-05-move-wicds-icon-to-gnome-shells-status-bar-top-bar/</guid><description>After I saw a extension of gnome-shell to move pidgin&amp;rsquo;s icon to the status bar, I tried to roll my own extension following the extension&amp;rsquo;s source to move the wicd&amp;rsquo;s icon to the status bar too. I followed the post Adding pidgin icon to gnome-shell status tray.
All I did is just to change the name of pidgin to wicd. After reboot my gnome-shell, it worked. Wicd&amp;rsquo;s icon appeared in status bar(top bar).</description></item><item><title>Python urllib2的timeout的问题</title><link>http://blog.allsunday.io/post/2011-11-29-python-set-urllib2-timeout/</link><pubDate>Tue, 29 Nov 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-11-29-python-set-urllib2-timeout/</guid><description>** 参考How Python’s urllib2 Bit Me **
python的 urllib2 包默认并不设置 timeout 的时间，它把这个问题向下传递到 httplib 包，而 httplib 包自己也不处理这个问题， 它也向下传递，到了 socket 库。由于 socket 是非常通用的一个库，从进程调用到抓取网页都会用到，所以它不知道该给 timeout 设置成什么，于是干脆设置了 forever。然后，就导致了在抓取网页的时候整个程序hang住。
解决的方法也很简单，在urllib2.open的时候加上timeout参数就ok了。
headers = {} url = '...' request = urllib2.Request(url, None, headers) response = urllib2.urlopen(request, timeout=60)</description></item><item><title>debian6上nginx与uwsgi安装</title><link>http://blog.allsunday.io/post/2011-11-24-debian6shang-nginxyu-uwsgian-zhuang/</link><pubDate>Thu, 24 Nov 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-11-24-debian6shang-nginxyu-uwsgian-zhuang/</guid><description>见linode的文档</description></item><item><title>debian安装mysql-python模块</title><link>http://blog.allsunday.io/post/2011-11-24-debianan-zhuang-mysql-pythonmo-kuai/</link><pubDate>Thu, 24 Nov 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-11-24-debianan-zhuang-mysql-pythonmo-kuai/</guid><description>直接 pip install mysql-python 提示 mysql-config 未找到，google了下，发现要先安装libmysql++-dev。
apt-get install limysql++-dev pip install mysql-python 安装完成，可以在python中用mysql了。</description></item><item><title>Emacs find file in git repository</title><link>http://blog.allsunday.io/post/2011-11-24-emacs-find-file-in-git-repository/</link><pubDate>Thu, 24 Nov 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-11-24-emacs-find-file-in-git-repository/</guid><description>{% gist 1391392 %}</description></item><item><title>Emacs笔记</title><link>http://blog.allsunday.io/post/2011-11-24-emacsbi-ji/</link><pubDate>Thu, 24 Nov 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-11-24-emacsbi-ji/</guid><description>&lt;h2 id="buffer操作">Buffer操作&lt;/h2>
&lt;ul>
&lt;li>在新buffer打开文件 C-x C-f&lt;/li>
&lt;li>在当前buffer打开文件 C-x C-v&lt;/li>
&lt;li>向当前buffer追加文件 C-x i&lt;/li>
&lt;li>另存为 C-x C-w&lt;/li>
&lt;/ul></description></item><item><title>Github整合Pull Request和Issue</title><link>http://blog.allsunday.io/post/2011-11-24-githubzheng-he-pull-requesthe-issue/</link><pubDate>Thu, 24 Nov 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-11-24-githubzheng-he-pull-requesthe-issue/</guid><description>我们把Github的Issue功能用来做计划，每次一个新功能完成，都会提交一个Pull Request。 提交一个Pull Request会自动新建一个Issue，但是原来已经存在一个Issue了，这样就有2 个相同内容的Issue。今天Google了下，发现只要的提交Pull Request的时候，在Commit信息 里面加入
close, closes, closed, fixes, fixed 这些关键词，比如 &amp;ldquo;this commit fixes #116&amp;rdquo;，Github会自动关联Issue和Pull Request。</description></item><item><title>Linux下用proxychains给所有的程序用上代理</title><link>http://blog.allsunday.io/post/2011-11-24-linuxxia-yong-proxychainsgei-suo-you-de-cheng-xu-yong-shang-dai-li/</link><pubDate>Thu, 24 Nov 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-11-24-linuxxia-yong-proxychainsgei-suo-you-de-cheng-xu-yong-shang-dai-li/</guid><description>Archlinux 下，只需要yaourt -S proxychains就能安装完毕，然后编辑/etc/proxychains.conf，修改proxy的地址为127.0.0.1，端口为7070。
然后在本地开启ssh代理，ssh -CfNg -D 127.0.0.1:7070 gfreezy@norida.me。一切的准备工作就完成了。
以后所有需要通过ssh代理来访问网络的程序，只要在命令前面加proxychains就可以了。如访问twitter，只需proxychains wget twitter.com就能获得twitter的首页了。</description></item><item><title>uwsgi与nginx的多站点配置</title><link>http://blog.allsunday.io/post/2011-11-24-uwsgiyu-nginxde-duo-zhan-dian-pei-zhi/</link><pubDate>Thu, 24 Nov 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-11-24-uwsgiyu-nginxde-duo-zhan-dian-pei-zhi/</guid><description>From uWSGI Examples
We have two virtualenv, DJANGOVENVS/pinax1 and DJANGOVENVS/pinax2.
Each virtualenv contains a pinax site in the directory &amp;lsquo;pinaxsite&amp;rsquo; and a script (called pinax.py) in every pinaxsite/deploy directory (that is a copy of deploy/pinax.wsgi)
Now configure nginx
server { listen 8080; server_name sirius.local; location / { include uwsgi_params; uwsgi_pass 127.0.0.1:3031; uwsgi_param UWSGI_PYHOME /Users/roberto/DJANGOVENVS/pinax1; uwsgi_param UWSGI_SCRIPT deploy.pinax; uwsgi_param UWSGI_CHDIR /Users/roberto/DJANGOVENVS/pinax1/pinaxsite; } } server { listen 8080; server_name localhost; location / { include uwsgi_params; uwsgi_pass 127.</description></item><item><title>win下设置emacs的默认编码</title><link>http://blog.allsunday.io/post/2011-11-24-winxia-she-zhi-emacsde-mo-ren-bian-ma/</link><pubDate>Thu, 24 Nov 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-11-24-winxia-she-zhi-emacsde-mo-ren-bian-ma/</guid><description>Emacs只能指定新建buffer的默认编码，和读取文件时候的编码寻找顺序。文件写入编码 Emacs会根据文件内容来猜测，并保持原有的编码不变。如想改变写入编码，需手动改变。
(set-buffer-file-coding-system 'utf-8-unix) ;;指定当前buffer的写入编码，只对当前buffer有效，即此命令写在配置文件中无效，只能 通过M-x来执行 (setq default-buffer-file-coding-system 'utf-8-unix) ;;指定新建buffer的默认编码为utf-8-unix，换行符为unix的方式 (prefer-coding-system 'utf-8) ;;将utf-8放到编码顺序表的最开始，即先从utf-8开始识别编码，此命令可以多次使用，后 指定的编码先探测 (set-language-environment 'utf-8) ;;指定Emacs的语言环境，按照特定语言环境设置前面的两个变量 Windows默认情况下，可以识别中文，也可以输入中文，但是新建文件的编码为 chinese-gbk-dos，为了改为utf-8，并且换行符为unix格式，在配置文件中加入下面这行。
(setq default-buffer-file-coding-system 'utf-8-unix) 附上手动修改编码的方法：
M-x set-buffer-file-coding-system coding 保存后，文件即是coding编码 C-x f coding 保存后，文件为coding编码 C-x r coding 以coding编码重新读取文件 C-x c coding 以coding编码执行接下去输入的命令，如 C-x c utf-8 C-x C-f a.txt 用utf-8编码打开a.txt文件</description></item><item><title>css中position属性和margin设置</title><link>http://blog.allsunday.io/post/2011-10-23-setting-position-and-margin-in-css/</link><pubDate>Sun, 23 Oct 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-10-23-setting-position-and-margin-in-css/</guid><description>position有static(默认),relative,fixed,absolute几个属性，他们作用分别如下：
static是默认属性，元素位置根据元素在文档中的位置来定位 relative属性下，可以用top，left，right，bottom来定位。这个定位的依据是文档中原来的位置，即相对于static属性下的位置来定位。它在文档中的位置保留不变，可以覆盖其他元素。 fixed属性下，根据浏览器窗口来定位，通过top，left，right，bottom来定位 absolute属性是相对于上一个设置absolute属性的元素来定位，也是top，right，left，bottom来定位 margin设置为负时，此元素即随后的所有元素都会发生位移，此偏移不是相对文档中的位置，而是文档中的位置之间发生变化。 使用margin时，如果没有设置position属性，marging会发生重叠，设置后不会发生重叠。</description></item><item><title>css和jquery里面的height属性</title><link>http://blog.allsunday.io/post/2011-10-23-height-in-css-and-jquery/</link><pubDate>Sun, 23 Oct 2011 00:00:00 +0000</pubDate><guid>http://blog.allsunday.io/post/2011-10-23-height-in-css-and-jquery/</guid><description> 获取浏览器窗口的可见高度用 document.body.clientHeight 获取当前网页的高度用 document.height height %100是相对于父元素的高度来设置的。
jquery获取height属性的几个方法
$('').height() 只获取元素的大小，不包括padding，border，margin $('').innerHeight() 包括padding $('').outerHeight() 包括padding和border $('').outerHeight(true) 包括padding，border，margin</description></item></channel></rss>